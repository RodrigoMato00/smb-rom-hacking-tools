# Inteligencia Artificial para Super Mario Bros

Este documento explica en detalle el sistema de Inteligencia Artificial utilizado para entrenar un agente que juega Super Mario Bros mediante Reinforcement Learning (Aprendizaje por Refuerzo).

---

## üìã √çndice

1. [¬øQu√© tipo de IA se usa?](#qu√©-tipo-de-ia-se-usa)
2. [¬øQu√© es Reinforcement Learning?](#qu√©-es-reinforcement-learning)
3. [Tecnolog√≠as y Bibliotecas](#tecnolog√≠as-y-bibliotecas)
4. [Algoritmo: PPO (Proximal Policy Optimization)](#algoritmo-ppo-proximal-policy-optimization)
5. [Arquitectura del Sistema](#arquitectura-del-sistema)
6. [Proceso de Entrenamiento](#proceso-de-entrenamiento)
7. [Scripts Disponibles](#scripts-disponibles)
8. [Configuraci√≥n y Par√°metros](#configuraci√≥n-y-par√°metros)
9. [C√≥mo Funciona el Entrenamiento](#c√≥mo-funciona-el-entrenamiento)
10. [Resultados Esperados](#resultados-esperados)

---

## ¬øQu√© tipo de IA se usa?

Este proyecto utiliza **Reinforcement Learning (Aprendizaje por Refuerzo)**, una rama del Machine Learning donde un agente aprende a tomar decisiones mediante la interacci√≥n con un entorno.

### Caracter√≠sticas clave:
- **Aprendizaje supervisado**: ‚ùå No requiere datos etiquetados
- **Aprendizaje no supervisado**: ‚ùå No busca patrones ocultos en datos
- **Reinforcement Learning**: ‚úÖ El agente aprende mediante prueba y error, recibiendo recompensas (rewards) por acciones exitosas

### Analog√≠a simple:
Imagina ense√±arle a un perro trucos:
- Le muestras un truco ‚Üí **Estado** (situaci√≥n actual)
- El perro intenta algo ‚Üí **Acci√≥n**
- Le das un premio si lo hizo bien ‚Üí **Recompensa positiva**
- Lo rega√±as si lo hizo mal ‚Üí **Recompensa negativa**
- Con el tiempo, el perro aprende qu√© acciones funcionan mejor ‚Üí **Pol√≠tica aprendida**

En nuestro caso:
- **Estado**: La pantalla actual del juego (imagen)
- **Acci√≥n**: Saltar, moverse a la derecha, etc.
- **Recompensa**: Avanzar en el nivel (+), perder una vida (-)
- **Pol√≠tica**: La estrategia que aprende la IA para jugar

---

## ¬øQu√© es Reinforcement Learning?

Reinforcement Learning es un paradigma de Machine Learning donde:

### Componentes principales:

1. **Agente (Agent)**
   - La IA que toma decisiones
   - En nuestro caso: una red neuronal

2. **Entorno (Environment)**
   - El mundo con el que el agente interact√∫a
   - En nuestro caso: Super Mario Bros emulado

3. **Estado (State/Observation)**
   - La informaci√≥n que el agente recibe del entorno
   - En nuestro caso: la imagen de cada frame del juego (240x256 p√≠xeles RGB)

4. **Acci√≥n (Action)**
   - Lo que el agente decide hacer en cada paso
   - En nuestro caso: una de 7 acciones posibles (NOOP, saltar, moverse derecha, etc.)

5. **Recompensa (Reward)**
   - Feedback num√©rico que indica qu√© tan buena fue la acci√≥n
   - En Super Mario Bros: avanza en X (+), pierde vida (-), mata enemigo (+), etc.

6. **Pol√≠tica (Policy)**
   - La estrategia que el agente aprende: "dado un estado, ¬øqu√© acci√≥n debo tomar?"
   - Representada por una red neuronal

### El ciclo de Reinforcement Learning:

```
Estado actual (imagen del juego)
    ‚Üì
Agente decide acci√≥n (ej: saltar)
    ‚Üì
Ejecuta acci√≥n en el entorno
    ‚Üì
Recibe nueva observaci√≥n + recompensa
    ‚Üì
Aprende: "¬øfue buena o mala esta decisi√≥n?"
    ‚Üì
Ajusta su pol√≠tica para tomar mejores decisiones en el futuro
    ‚Üì
(Repite millones de veces)
```

---

## Tecnolog√≠as y Bibliotecas

### Stack tecnol√≥gico completo:

#### 1. **gym_super_mario_bros** (v7.3.0)
- **Qu√© es**: Entorno de OpenAI Gym espec√≠fico para Super Mario Bros
- **Funci√≥n**: Proporciona la interfaz entre Python y el emulador del juego
- **Caracter√≠sticas**:
  - Emula Super Mario Bros usando `nes_py`
  - Proporciona observaciones (im√°genes del juego)
  - Recibe acciones y devuelve recompensas
  - Maneja el ciclo de reset/game over autom√°ticamente

#### 2. **nes_py** (v8.2.1)
- **Qu√© es**: Emulador NES en Python
- **Funci√≥n**: Ejecuta la ROM de Super Mario Bros y proporciona acceso a la RAM/estado del juego
- **Caracter√≠sticas**:
  - Emulaci√≥n completa del NES
  - Acceso a memoria del juego
  - Renderizado de frames

#### 3. **stable-baselines3** (v1.6.2)
- **Qu√© es**: Biblioteca de algoritmos de Reinforcement Learning
- **Funci√≥n**: Implementa PPO y otros algoritmos RL listos para usar
- **Caracter√≠sticas**:
  - Algoritmos optimizados y probados
  - Soporte para pol√≠ticas basadas en CNNs (para im√°genes)
  - Callbacks y logging integrados
  - Guardado/carga de modelos

#### 4. **PyTorch** (v1.13.1)
- **Qu√© es**: Framework de deep learning
- **Funci√≥n**: Proporciona la infraestructura para las redes neuronales
- **Caracter√≠sticas**:
  - Redes neuronales convolucionales (CNNs)
  - C√°lculo autom√°tico de gradientes
  - Optimizaci√≥n de par√°metros
  - Soporte para CPU y GPU

#### 5. **gym** (v0.21.0)
- **Qu√© es**: Est√°ndar de OpenAI para entornos de RL
- **Funci√≥n**: Define la interfaz com√∫n entre agentes y entornos
- **Caracter√≠sticas**:
  - API est√°ndar: `reset()`, `step()`, `render()`
  - Wrappers para modificar entornos

---

## Algoritmo: PPO (Proximal Policy Optimization)

### ¬øQu√© es PPO?

**PPO (Proximal Policy Optimization)** es un algoritmo de Reinforcement Learning propuesto por OpenAI en 2017. Es uno de los algoritmos m√°s populares y efectivos para entrenar agentes en juegos.

### ¬øPor qu√© PPO?

- ‚úÖ **Estable**: Menos propenso a "romperse" durante el entrenamiento
- ‚úÖ **Eficiente**: Aprende r√°pido con relativamente pocos datos
- ‚úÖ **Robusto**: Funciona bien en muchos entornos diferentes
- ‚úÖ **On-policy**: Aprende directamente de las interacciones actuales

### Conceptos clave de PPO:

#### 1. **Policy Gradient**
- La red neuronal (pol√≠tica) se entrena para maximizar la recompensa esperada
- Usa gradientes para ajustar los pesos de la red

#### 2. **Proximal**
- Limita qu√© tan "lejos" puede cambiar la pol√≠tica en cada actualizaci√≥n
- Esto evita que la pol√≠tica se rompa si actualiza demasiado r√°pido

#### 3. **Clipping**
- Si una actualizaci√≥n ser√≠a demasiado grande, la "recorta" (clip)
- Esto mantiene el entrenamiento estable

### Par√°metros de PPO en nuestro proyecto:

```python
PPO(
    'CnnPolicy',          # Pol√≠tica: CNN para procesar im√°genes
    env,                   # Entorno
    verbose=1,             # Logging
    n_steps=512,           # Pasos por actualizaci√≥n
    learning_rate=0.000001,# Tasa de aprendizaje (muy baja, aprendizaje lento pero estable)
    device="cpu"           # CPU o GPU
)
```

**Explicaci√≥n de par√°metros:**
- **n_steps=512**: El agente juega 512 pasos antes de actualizar la red
- **learning_rate=0.000001**: Qu√© tan r√°pido aprenden los pesos (muy conservador)
- **CnnPolicy**: Usa una Convolutional Neural Network para procesar im√°genes

---

## Arquitectura del Sistema

### Flujo completo:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ENTORNO (Super Mario Bros)                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ  ‚îÇ  Emulador   ‚îÇ ‚Üê‚îÄ‚îÄ‚Üí  ‚îÇ  Observaci√≥n ‚îÇ (imagen 240x256x3) ‚îÇ
‚îÇ  ‚îÇ   NES       ‚îÇ       ‚îÇ   (State)    ‚îÇ                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ       ‚Üë                         ‚îÇ                           ‚îÇ
‚îÇ       ‚îÇ                         ‚Üì                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ  ‚îÇ    Acci√≥n    ‚îÇ      ‚îÇ  Agente (Red Neur.) ‚îÇ             ‚îÇ
‚îÇ  ‚îÇ  (ej: saltar)‚îÇ ‚Üê‚îÄ‚îÄ‚îÄ ‚îÇ  Pol√≠tica (PPO)     ‚îÇ             ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îÇ       ‚Üë                         ‚îÇ                           ‚îÇ
‚îÇ       ‚îÇ                         ‚Üì                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ  ‚îÇ  Recompensa  ‚îÇ ‚Üê‚îÄ‚îÄ‚îÄ ‚îÇ  Ajuste de Pesos    ‚îÇ             ‚îÇ
‚îÇ  ‚îÇ   (Reward)   ‚îÇ      ‚îÇ  (Entrenamiento)    ‚îÇ             ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Arquitectura de la Red Neuronal:

La pol√≠tica usa una **CNN (Convolutional Neural Network)** con esta estructura:

```
Input: Imagen RGB (240x256x3)
    ‚Üì
Convoluci√≥n 1: Extrae caracter√≠sticas b√°sicas (bordes, formas)
    ‚Üì
Convoluci√≥n 2: Detecta patrones m√°s complejos
    ‚Üì
Convoluci√≥n 3: Reconoce objetos (Mario, enemigos, bloques)
    ‚Üì
Flatten: Convierte a vector 1D
    ‚Üì
Capa Densa 1: Procesa informaci√≥n abstracta
    ‚Üì
Capa Densa 2: Toma decisi√≥n
    ‚Üì
Output: Probabilidades para cada acci√≥n (7 acciones posibles)
```

### Acciones disponibles:

Usamos `SIMPLE_MOVEMENT` que proporciona 7 acciones:

```python
[
    ['NOOP'],           # 0: No hacer nada
    ['right'],          # 1: Mover derecha
    ['right', 'A'],     # 2: Mover derecha + saltar
    ['right', 'B'],     # 3: Mover derecha + correr
    ['right', 'A', 'B'],# 4: Mover derecha + saltar + correr
    ['A'],              # 5: Saltar
    ['left']            # 6: Mover izquierda
]
```

---

## Proceso de Entrenamiento

### Fase 1: Recolecci√≥n de Experiencias (Rollout)

1. El agente (con pol√≠tica inicial aleatoria) juega el juego
2. Para cada frame:
   - Observa la imagen actual
   - Decide una acci√≥n usando la pol√≠tica actual
   - Ejecuta la acci√≥n
   - Recibe nueva observaci√≥n + recompensa
   - Guarda la "experiencia": (estado, acci√≥n, recompensa, siguiente_estado)

3. Recolecta **n_steps** (512) experiencias

### Fase 2: C√°lculo de Ventajas (Advantage)

- Calcula qu√© tan "buena" fue cada decisi√≥n comparada con el promedio
- Ventaja positiva = acci√≥n mejor que el promedio ‚Üí aumentar probabilidad
- Ventaja negativa = acci√≥n peor que el promedio ‚Üí disminuir probabilidad

### Fase 3: Actualizaci√≥n de la Pol√≠tica

- Usa las experiencias y ventajas para actualizar los pesos de la red
- Ajusta la pol√≠tica para aumentar la probabilidad de acciones "buenas"
- Usa "clipping" para evitar cambios demasiado grandes

### Fase 4: Repetici√≥n

- Repite Fase 1-3 millones de veces
- Con cada iteraci√≥n, la pol√≠tica mejora gradualmente

### Ejemplo simplificado:

```
Iteraci√≥n 1:
  - Pol√≠tica: Aleatoria
  - Mario salta al azar, muere r√°pido
  - Recompensa promedio: -50

Iteraci√≥n 100:
  - Pol√≠tica: Aprendi√≥ que mover derecha es bueno
  - Mario avanza un poco antes de morir
  - Recompensa promedio: 200

Iteraci√≥n 1000:
  - Pol√≠tica: Aprendi√≥ a saltar cuando hay enemigo
  - Mario sobrevive m√°s tiempo
  - Recompensa promedio: 500

Iteraci√≥n 10000:
  - Pol√≠tica: Aprendi√≥ estrategias complejas
  - Mario completa niveles
  - Recompensa promedio: 2000+
```

---

## Scripts Disponibles

### 1. `rl_demo_mario.py`

**Funci√≥n**: Entrenar y jugar con la ROM est√°ndar de Super Mario Bros.

**Uso b√°sico:**
```bash
# Entrenar (default: 15000 timesteps)
python scripts/rl_demo_mario.py

# Entrenar con m√°s timesteps
python scripts/rl_demo_mario.py --timesteps 50000

# Jugar con modelo entrenado
python scripts/rl_demo_mario.py --load mario_ppo_model.zip
```

**Caracter√≠sticas:**
- Usa la ROM est√°ndar incluida en `gym_super_mario_bros`
- Configuraci√≥n r√°pida para pruebas
- Par√°metros: `n_steps=256`, `learning_rate=0.0003`

### 2. `mario_rl_custom_rom.py`

**Funci√≥n**: Entrenar y jugar con ROMs personalizadas (ej: Mario invencible, cielo nocturno, etc.)

**Uso b√°sico:**
```bash
# Entrenar con ROM personalizada (default: 1000000 timesteps)
python scripts/mario_rl_custom_rom.py roms/SuperMarioBros_star_infinite_20251028_231603.nes

# Jugar con modelo entrenado
python scripts/mario_rl_custom_rom.py roms/SuperMarioBros_star_infinite_20251028_231603.nes --load mario_invencible_model.zip
```

**Caracter√≠sticas:**
- Permite usar cualquier ROM modificada
- Corrige autom√°ticamente el header de la ROM para compatibilidad
- Hace backup y restauraci√≥n autom√°tica de la ROM original
- Par√°metros seg√∫n tutorial: `n_steps=512`, `learning_rate=0.000001`
- Entrenamiento largo (1,000,000 timesteps) para mejor aprendizaje

**IMPORTANTE**: Si usas una ROM modificada, debes **reentrenar** el modelo con esa ROM. Un modelo entrenado con ROM normal no sabe aprovechar las modificaciones (ej: invencibilidad).

---

## Configuraci√≥n y Par√°metros

### Entorno Python requerido:

**Versi√≥n**: Python 3.8 (requerido por `gym-retro` y compatibilidad)

**Dependencias espec√≠ficas:**
```bash
pip install gym==0.21.0
pip install gym_super_mario_bros==7.3.0
pip install nes_py==8.2.1
pip install stable-baselines3==1.6.2
pip install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1
```

### Par√°metros de entrenamiento:

#### Configuraci√≥n r√°pida (`rl_demo_mario.py`):
```python
PPO(
    'CnnPolicy',
    env,
    n_steps=256,           # Menos pasos por actualizaci√≥n (m√°s r√°pido)
    learning_rate=0.0003,  # Tasa m√°s alta (aprende m√°s r√°pido)
    batch_size=256,
    device="cpu"
)
model.learn(total_timesteps=15000)  # Entrenamiento corto
```

#### Configuraci√≥n completa (`mario_rl_custom_rom.py`):
```python
PPO(
    'CnnPolicy',
    env,
    n_steps=512,           # M√°s pasos por actualizaci√≥n (m√°s estable)
    learning_rate=0.000001,# Tasa muy baja (aprendizaje lento pero estable)
    device="cpu"
)
model.learn(total_timesteps=1000000)  # Entrenamiento largo
```

### Diferencias entre configuraciones:

| Par√°metro | R√°pida | Completa |
|-----------|--------|----------|
| **n_steps** | 256 | 512 |
| **learning_rate** | 0.0003 | 0.000001 |
| **total_timesteps** | 15,000 | 1,000,000 |
| **Tiempo estimado** | ~1 min | ~24-48 horas |
| **Calidad del modelo** | B√°sico | Avanzado |

---

## C√≥mo Funciona el Entrenamiento

### Paso a paso detallado:

#### 1. Inicializaci√≥n
```python
env = gym_super_mario_bros.make("SuperMarioBros-v0")
env = JoypadSpace(env, SIMPLE_MOVEMENT)
```
- Crea el entorno del juego
- Limita a 7 acciones simples

#### 2. Creaci√≥n del modelo
```python
model = PPO("CnnPolicy", env, ...)
```
- Inicializa la red neuronal con pesos aleatorios
- La pol√≠tica inicial es completamente aleatoria (no sabe jugar)

#### 3. Loop de entrenamiento

**Por cada iteraci√≥n (512 pasos):**

```python
# Recolectar experiencias
for step in range(512):
    obs = env.get_current_screen()  # Imagen RGB
    action = model.predict(obs)      # Decisi√≥n de la red
    obs_new, reward, done = env.step(action)
    # Guarda: (obs, action, reward, obs_new)
```

#### 4. C√°lculo de ventajas

```python
# Usa las recompensas recibidas para calcular:
advantage = reward - valor_esperado
# Si advantage > 0: acci√≥n fue buena
# Si advantage < 0: acci√≥n fue mala
```

#### 5. Actualizaci√≥n de la red

```python
# Ajusta los pesos para:
# - Aumentar probabilidad de acciones con advantage positivo
# - Disminuir probabilidad de acciones con advantage negativo
model.update(experiences, advantages)
```

#### 6. Repetici√≥n

- Repite pasos 3-5 hasta completar **total_timesteps**
- Con 1,000,000 timesteps: ~1953 iteraciones completas

### Qu√© aprende el agente en cada etapa:

**Etapa inicial (0-10,000 timesteps):**
- Aprende conceptos b√°sicos: "mover derecha avanza"
- Muere frecuentemente
- Recompensa: -50 a 100

**Etapa intermedia (10,000-100,000 timesteps):**
- Aprende a saltar sobre obst√°culos peque√±os
- Sobrevive m√°s tiempo
- Recompensa: 200-500

**Etapa avanzada (100,000-1,000,000 timesteps):**
- Aprende estrategias complejas
- Evita enemigos eficientemente
- Completa niveles
- Recompensa: 1000-5000+

---

## M√©tricas de Entrenamiento

Durante el entrenamiento, PPO muestra estas m√©tricas:

### M√©tricas de tiempo:
- **fps**: Frames por segundo (velocidad de simulaci√≥n)
- **iterations**: N√∫mero de actualizaciones completadas
- **time_elapsed**: Tiempo total de entrenamiento
- **total_timesteps**: Pasos totales ejecutados

### M√©tricas de entrenamiento:
- **approx_kl**: Divergencia KL aproximada (mide cambio en pol√≠tica)
  - Valores altos = pol√≠tica cambiando mucho
  - Valores bajos = pol√≠tica cambiando poco

- **clip_fraction**: Fracci√≥n de actualizaciones "recortadas"
  - Indica qu√© tan restrictiva es la actualizaci√≥n

- **entropy_loss**: Entrop√≠a de la pol√≠tica
  - Valores altos = pol√≠tica explora mucho (aleatoria)
  - Valores bajos = pol√≠tica es determinista

- **explained_variance**: Qu√© tan bien predice el valor
  - Valores cercanos a 1 = buenas predicciones
  - Valores negativos = predicciones peores que el promedio

- **loss**: P√©rdida total del modelo
  - Disminuye con entrenamiento exitoso

- **policy_gradient_loss**: P√©rdida de la pol√≠tica
  - Mide qu√© tan bien se ajusta la pol√≠tica

- **value_loss**: P√©rdida del valor estimado
  - Mide qu√© tan bien estima recompensas futuras

### Interpretaci√≥n:

Un entrenamiento saludable muestra:
- ‚úÖ `loss` disminuyendo gradualmente
- ‚úÖ `explained_variance` acerc√°ndose a 1
- ‚úÖ `entropy_loss` disminuyendo (pol√≠tica se hace m√°s segura)
- ‚úÖ `approx_kl` en rango razonable (no demasiado alto)

---

## Resultados Esperados

### Con entrenamiento corto (15,000 timesteps):
- ‚è±Ô∏è Tiempo: ~1-2 minutos
- üéÆ Resultado: Aprende movimientos b√°sicos
- ‚ö†Ô∏è Limitaciones: No salta obst√°culos consistentemente, muere frecuentemente

### Con entrenamiento completo (1,000,000 timesteps):
- ‚è±Ô∏è Tiempo: ~24-48 horas (depende del hardware)
- üéÆ Resultado: Juega de forma competente
- ‚úÖ Habilidades aprendidas:
  - Saltar sobre enemigos
  - Evitar obst√°culos
  - Avanzar eficientemente
  - Completar niveles b√°sicos
  - En ROM invencible: Aprovecha la invencibilidad para avanzar sin miedo

---

## Fixes T√©cnicos Implementados

### 1. Problema de "Negative Strides"

**Problema**: PyTorch no acepta arrays NumPy con strides negativos.

**Soluci√≥n**: Funci√≥n `ensure_contiguous()`:
```python
def ensure_contiguous(obs: np.ndarray) -> np.ndarray:
    """Devuelve una copia contigua lista para pasar a torch.as_tensor()"""
    if isinstance(obs, np.ndarray) and not obs.flags.c_contiguous:
        return np.ascontiguousarray(obs)
    return obs
```

### 2. Problema de Batch Dimension

**Problema**: Stable-Baselines3 espera observaciones con dimensi√≥n de batch.

**Soluci√≥n**: Expandir dimensi√≥n si es necesario:
```python
if obs_input.ndim == 3:  # (H, W, C)
    obs_input = np.expand_dims(obs_input, axis=0)  # -> (1, H, W, C)
```

### 3. Problema de Tipo de Acci√≥n

**Problema**: `model.predict()` devuelve un array, pero `JoypadSpace` espera un entero.

**Soluci√≥n**: Convertir a entero:
```python
if isinstance(action, np.ndarray):
    action = int(action.item())
```

### 4. Header de ROM

**Problema**: `nes_py` requiere que bytes  aventurero15 del header sean cero.

**Soluci√≥n**: Correcci√≥n autom√°tica del header antes de usar la ROM.

---

## Casos de Uso

### Entrenar modelo para ROM normal:
```bash
python scripts/rl_demo_mario.py --timesteps 1000000
```

### Entrenar modelo para ROM invencible:
```bash
python scripts/mario_rl_custom_rom.py roms/SuperMarioBros_star_infinite_20251028_231603.nes --timesteps 1000000
```

### Demo con modelo entrenado:
```bash
python scripts/mario_rl_custom_rom.py roms/SuperMarioBros_star_infinite_20251028_231603.nes --load mario_invencible_model.zip --seconds 300
```

---

## Consideraciones Finales

### Ventajas de este enfoque:
- ‚úÖ No requiere datos de entrenamiento pre-etiquetados
- ‚úÖ El agente aprende explorando por s√≠ mismo
- ‚úÖ Puede descubrir estrategias no obvias
- ‚úÖ Funciona con cualquier ROM (con reentrenamiento)

### Limitaciones:
- ‚è±Ô∏è Entrenamiento requiere mucho tiempo
- üíª Consume recursos computacionales significativos
- üéØ Requiere ajuste fino de hiperpar√°metros para mejores resultados
- üéÆ La calidad depende mucho del tiempo de entrenamiento

### Mejoras posibles:
- Usar GPU para acelerar entrenamiento
- Ajustar hiperpar√°metros (learning rate, n_steps, etc.)
- Implementar reward shaping (dise√±ar mejores recompensas)
- Usar algoritmos m√°s avanzados (SAC, TD3, etc.)
- Entrenar con m√∫ltiples niveles para generalizaci√≥n

---

*Documentaci√≥n generada para el proyecto de ROM hacking y Reinforcement Learning con Super Mario Bros*

